{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "took 0.56 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "def test():\n",
    "    t0 = time.time()\n",
    "\n",
    "    topic_name = 'test-topic'\n",
    "\n",
    "    for i in range(10):\n",
    "        message = {'number': i}\n",
    "        producer.send(topic_name, value=message)\n",
    "        print(f\"Sent: {message}\")\n",
    "        time.sleep(0.05)\n",
    "\n",
    "    producer.flush()\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent: {'number': 0}\n",
      "Sent: {'number': 1}\n",
      "Sent: {'number': 2}\n",
      "Sent: {'number': 3}\n",
      "Sent: {'number': 4}\n",
      "Sent: {'number': 5}\n",
      "Sent: {'number': 6}\n",
      "Sent: {'number': 7}\n",
      "Sent: {'number': 8}\n",
      "Sent: {'number': 9}\n",
      "took 0.55 seconds\n",
      "         1531 function calls in 0.548 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    0.548    0.548 1638414867.py:2(test)\n",
      "       10    0.000    0.000    0.000    0.000 3926510068.py:6(json_serializer)\n",
      "       10    0.000    0.000    0.000    0.000 <string>:1(<lambda>)\n",
      "        1    0.000    0.000    0.548    0.548 <string>:1(<module>)\n",
      "       31    0.000    0.000    0.000    0.000 __init__.py:1467(debug)\n",
      "       31    0.000    0.000    0.000    0.000 __init__.py:1734(isEnabledFor)\n",
      "       10    0.000    0.000    0.000    0.000 __init__.py:183(dumps)\n",
      "       10    0.000    0.000    0.000    0.000 buffer.py:38(allocate)\n",
      "       11    0.000    0.000    0.001    0.000 client_async.py:929(wakeup)\n",
      "       20    0.000    0.000    0.000    0.000 cluster.py:106(partitions_for_topic)\n",
      "       10    0.000    0.000    0.000    0.000 cluster.py:119(available_partitions_for_topic)\n",
      "       10    0.000    0.000    0.000    0.000 cluster.py:131(<listcomp>)\n",
      "       10    0.000    0.000    0.000    0.000 default.py:15(__call__)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:376(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:406(append)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:545(size)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:563(size_of)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:591(estimate_size_in_bytes)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:605(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:610(offset)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:614(crc)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:618(size)\n",
      "       10    0.000    0.000    0.000    0.000 default_records.py:622(timestamp)\n",
      "       10    0.000    0.000    0.000    0.000 encoder.py:183(encode)\n",
      "       10    0.000    0.000    0.000    0.000 encoder.py:205(iterencode)\n",
      "       10    0.000    0.000    0.000    0.000 future.py:11(__init__)\n",
      "       20    0.000    0.000    0.000    0.000 future.py:12(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 future.py:32(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 future.py:48(add_callback)\n",
      "       10    0.000    0.000    0.000    0.000 future.py:57(add_errback)\n",
      "        3    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "        3    0.000    0.000    0.001    0.000 iostream.py:259(schedule)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "       22    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "       22    0.000    0.000    0.001    0.000 iostream.py:577(_schedule_flush)\n",
      "       22    0.000    0.000    0.002    0.000 iostream.py:655(write)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:521(_max_usable_produce_magic)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:529(_estimate_size_in_bytes)\n",
      "       10    0.000    0.000    0.005    0.000 kafka.py:538(send)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:593(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 kafka.py:624(flush)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:651(_ensure_valid_record_size)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:664(_wait_on_metadata)\n",
      "       20    0.000    0.000    0.000    0.000 kafka.py:709(_serialize)\n",
      "       10    0.000    0.000    0.000    0.000 kafka.py:716(_partition)\n",
      "       10    0.000    0.000    0.000    0.000 memory_records.py:118(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 memory_records.py:137(append)\n",
      "       10    0.000    0.000    0.000    0.000 memory_records.py:176(is_full)\n",
      "       10    0.000    0.000    0.000    0.000 random.py:235(_randbelow_with_getrandbits)\n",
      "       10    0.000    0.000    0.000    0.000 random.py:367(choice)\n",
      "       10    0.000    0.000    0.002    0.000 record_accumulator.py:200(append)\n",
      "       11    0.000    0.000    0.000    0.000 record_accumulator.py:24(increment)\n",
      "       11    0.000    0.000    0.000    0.000 record_accumulator.py:29(decrement)\n",
      "       10    0.000    0.000    0.000    0.000 record_accumulator.py:39(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 record_accumulator.py:513(begin_flush)\n",
      "        1    0.000    0.000    0.000    0.000 record_accumulator.py:520(await_flush_completion)\n",
      "       10    0.000    0.000    0.001    0.000 record_accumulator.py:57(try_append)\n",
      "       10    0.000    0.000    0.000    0.000 record_accumulator.py:580(add)\n",
      "        1    0.000    0.000    0.000    0.000 record_accumulator.py:588(all)\n",
      "       10    0.000    0.000    0.000    0.000 sender.py:173(add_topic)\n",
      "       11    0.000    0.000    0.001    0.000 sender.py:331(wakeup)\n",
      "       10    0.000    0.000    0.000    0.000 six.py:592(iteritems)\n",
      "        3    0.001    0.000    0.001    0.000 socket.py:621(send)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1118(_wait_for_tstate_lock)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1185(is_alive)\n",
      "       10    0.000    0.000    0.000    0.000 threading.py:236(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 threading.py:555(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:568(is_set)\n",
      "        1    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "       50    0.000    0.000    0.000    0.000 util.py:10(encode_varint)\n",
      "       30    0.000    0.000    0.000    0.000 util.py:63(size_of_varint)\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x1038cbc70}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        1    0.000    0.000    0.548    0.548 {built-in method builtins.exec}\n",
      "       62    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "      132    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "       20    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "       11    0.000    0.000    0.002    0.000 {built-in method builtins.print}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method posix.getpid}\n",
      "       10    0.541    0.054    0.541    0.054 {built-in method time.sleep}\n",
      "       42    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       32    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "       64    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "       60    0.000    0.000    0.000    0.000 {method 'append' of 'bytearray' objects}\n",
      "       13    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'bit_length' of 'int' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'extend' of 'bytearray' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'getrandbits' of '_random.Random' objects}\n",
      "       32    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method 'keys' of 'dict' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "       11    0.001    0.000    0.001    0.000 {method 'sendall' of '_socket.socket' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cProfile.run('test()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import requests \n",
    "import gzip\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2024-03-13 21:37:10--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n",
      "Resolving github.com (github.com)... 20.205.243.166\n",
      "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240313%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240313T143713Z&X-Amz-Expires=300&X-Amz-Signature=b665b4e074ca77e0538c03403d7eaf2083ffdc8086c4766f610b2e30a6353262&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2024-03-13 21:37:11--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/ea580e9e-555c-4bd0-ae73-43051d8e7c0b?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAVCODYLSA53PQK4ZA%2F20240313%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240313T143713Z&X-Amz-Expires=300&X-Amz-Signature=b665b4e074ca77e0538c03403d7eaf2083ffdc8086c4766f610b2e30a6353262&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dgreen_tripdata_2019-10.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8262584 (7.9M) [application/octet-stream]\n",
      "Saving to: ‘green_tripdata_2019-10.csv.gz’\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 4.14M 2s\n",
      "    50K .......... .......... .......... .......... ..........  1% 11.1M 1s\n",
      "   100K .......... .......... .......... .......... ..........  1% 7.78M 1s\n",
      "   150K .......... .......... .......... .......... ..........  2% 16.5M 1s\n",
      "   200K .......... .......... .......... .......... ..........  3% 14.5M 1s\n",
      "   250K .......... .......... .......... .......... ..........  3% 6.91M 1s\n",
      "   300K .......... .......... .......... .......... ..........  4% 3.37M 1s\n",
      "   350K .......... .......... .......... .......... ..........  4% 23.6M 1s\n",
      "   400K .......... .......... .......... .......... ..........  5% 7.61M 1s\n",
      "   450K .......... .......... .......... .......... ..........  6% 5.71M 1s\n",
      "   500K .......... .......... .......... .......... ..........  6% 7.86M 1s\n",
      "   550K .......... .......... .......... .......... ..........  7% 7.21M 1s\n",
      "   600K .......... .......... .......... .......... ..........  8% 6.39M 1s\n",
      "   650K .......... .......... .......... .......... ..........  8% 7.38M 1s\n",
      "   700K .......... .......... .......... .......... ..........  9% 5.33M 1s\n",
      "   750K .......... .......... .......... .......... ..........  9% 6.40M 1s\n",
      "   800K .......... .......... .......... .......... .......... 10% 7.14M 1s\n",
      "   850K .......... .......... .......... .......... .......... 11% 5.88M 1s\n",
      "   900K .......... .......... .......... .......... .......... 11% 8.24M 1s\n",
      "   950K .......... .......... .......... .......... .......... 12% 6.42M 1s\n",
      "  1000K .......... .......... .......... .......... .......... 13% 7.47M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 13% 7.71M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 14% 5.87M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 14% 5.23M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 15% 8.61M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 16% 7.04M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 16% 6.08M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 17% 7.30M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 17% 9.52M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 18% 5.04M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 19% 8.25M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 19% 6.35M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 20% 6.11M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 21% 5.10M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 21% 7.61M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 22% 7.51M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 22% 6.02M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 23% 8.30M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 24% 7.06M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 24% 6.34M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 25% 7.24M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 26% 5.13M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 26% 6.70M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 27% 7.81M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 27% 6.21M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 28% 8.22M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 29% 6.42M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 29% 6.81M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 30% 7.33M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 30% 5.28M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 31% 5.58M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 32% 9.29M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 32% 6.10M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 33% 7.52M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 34% 7.15M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 34% 6.11M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 35% 5.93M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 35% 7.43M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 36% 10.1M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 37% 6.80M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 37% 8.49M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 38% 9.15M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 39% 8.61M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 39% 6.53M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 40% 2.45M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 40% 54.1M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 41% 26.2M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 42%  209M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 42% 16.6M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 43% 8.15M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 43% 10.7M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 44% 6.89M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 45% 9.10M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 45% 12.0M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 46% 20.0M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 47% 14.6M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 47% 15.1M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 48% 40.2M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 48% 15.6M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 49% 10.9M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 50% 25.0M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 50%  432M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 51% 15.2M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 52% 13.9M 0s\n",
      "  4200K .......... .......... .......... .......... .......... 52% 18.7M 0s\n",
      "  4250K .......... .......... .......... .......... .......... 53% 33.8M 0s\n",
      "  4300K .......... .......... .......... .......... .......... 53% 18.0M 0s\n",
      "  4350K .......... .......... .......... .......... .......... 54% 14.0M 0s\n",
      "  4400K .......... .......... .......... .......... .......... 55% 13.9M 0s\n",
      "  4450K .......... .......... .......... .......... .......... 55%  112M 0s\n",
      "  4500K .......... .......... .......... .......... .......... 56% 12.2M 0s\n",
      "  4550K .......... .......... .......... .......... .......... 57% 23.1M 0s\n",
      "  4600K .......... .......... .......... .......... .......... 57% 19.5M 0s\n",
      "  4650K .......... .......... .......... .......... .......... 58% 60.7M 0s\n",
      "  4700K .......... .......... .......... .......... .......... 58% 9.16M 0s\n",
      "  4750K .......... .......... .......... .......... .......... 59% 19.2M 0s\n",
      "  4800K .......... .......... .......... .......... .......... 60% 13.8M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 60% 24.0M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 61% 18.4M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 61% 47.5M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 62% 15.6M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 63% 36.5M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 63% 12.7M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 64% 17.0M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 65% 34.7M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 65% 20.3M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 66% 17.9M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 66% 15.8M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 67% 18.2M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 68% 67.3M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 68% 25.0M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 69% 10.2M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 70% 22.1M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 70% 19.9M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 71% 28.6M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 71% 10.8M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 72% 17.4M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 73% 10.9M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 73% 13.2M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 74% 28.0M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 74% 12.0M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 75% 10.2M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 76% 18.0M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 76% 12.9M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 77% 13.6M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 78% 24.4M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 78% 10.9M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 79% 12.7M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 79% 11.7M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 80% 23.4M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 81% 11.3M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 81% 48.9M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 82% 13.5M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 83% 11.2M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 83% 16.1M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 84% 14.9M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 84% 12.0M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 85% 16.8M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 86% 15.5M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 86% 11.7M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 87% 24.2M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 87% 24.6M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 88% 20.6M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 89% 9.85M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 89% 23.1M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 90% 11.7M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 91% 17.9M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 91%  279M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 92% 7.38M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 92% 10.5M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 93% 8.84M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 94% 8.32M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 94% 7.29M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 95% 8.87M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 96% 11.1M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 96% 14.8M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 97% 8.99M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 97% 44.0M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 98% 18.7M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 99% 11.7M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 99% 10.9M 0s\n",
      "  8050K .......... ........                                   100%  342M=0.8s\n",
      "\n",
      "2024-03-13 21:37:13 (10.1 MB/s) - ‘green_tripdata_2019-10.csv.gz’ saved [8262584/8262584]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz\n",
    "gunzip green_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=['lpep_pickup_datetime', 'lpep_dropoff_datetime', 'PULocationID', \n",
    "        'DOLocationID', 'passenger_count', 'trip_distance', 'tip_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green = pd.read_csv('green_tripdata_2019-10.csv',index_col=False,usecols=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 31.52 seconds\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "topic_name = 'green-trips'\n",
    "for row in df_green.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "    #print(f\"Sent: {row_dict}\")\n",
    "\n",
    "producer.flush()\n",
    "\n",
    "t1 = time.time()\n",
    "print(f'took {(t1 - t0):.2f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:46:33 WARN Utils: Your hostname, PPM1.local resolves to a loopback address: 127.0.0.1; using 192.168.1.43 instead (on interface en0)\n",
      "24/03/14 11:46:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/ppathorn/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/ppathorn/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-fd50e5bf-ddf1-425a-8bd5-f80059a898d2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.1/spark-sql-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1!spark-sql-kafka-0-10_2.12.jar (1274ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.1/spark-token-provider-kafka-0-10_2.12-3.5.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1!spark-token-provider-kafka-0-10_2.12.jar (539ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (6656ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (616ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (608ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (11815ms)\n",
      "downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...\n",
      "\t[SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (6209ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (1018ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (621ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (7300ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...\n",
      "\t[SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (511ms)\n",
      ":: resolution report :: resolve 26740ms :: artifacts dl 37182ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   11  |   11  |   0   ||   11  |   11  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-fd50e5bf-ddf1-425a-8bd5-f80059a898d2\n",
      "\tconfs: [default]\n",
      "\t11 artifacts copied, 0 already retrieved (56767kB/68ms)\n",
      "24/03/14 11:47:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:48:20 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/rz/0gz_h6gx0cngzvm1_ml4k1lh0000gn/T/temporary-e722aa00-91e1-4bc2-878f-c132611a0410. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/14 11:48:20 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:48:20 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 14, 11, 45, 51, 120000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.show of DataFrame[lpep_pickup_datetime: string, lpep_dropoff_datetime: string, PULocationID: int, DOLocationID: int, passenger_count: double, trip_distance: double, tip_amount: double]>\n"
     ]
    }
   ],
   "source": [
    "print(green_stream.show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_with_timestamp = green_stream \\\n",
    "    .withColumn(\"timestamp\", F.current_timestamp())\n",
    "\n",
    "popular_destinations = stream_with_timestamp \\\n",
    "    .groupBy(F.window(F.col(\"timestamp\"), \"5 minutes\"), F.col(\"DOLocationID\")) \\\n",
    "    .count() \\\n",
    "    .orderBy(F.col(\"count\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/14 11:57:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /private/var/folders/rz/0gz_h6gx0cngzvm1_ml4k1lh0000gn/T/temporary-1ebf964b-460d-49e1-82b9-7e4733862396. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/14 11:57:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/03/14 11:57:13 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=38352Kb max_used=38369Kb free=92719Kb\n",
      " bounds [0x00000001089e0000, 0x000000010af90000, 0x00000001109e0000]\n",
      " total_blobs=13844 nmethods=12773 adapters=983\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------+-----+\n",
      "|window                                    |DOLocationID|count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|74          |17741|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|42          |15942|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|41          |14061|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|75          |12840|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|129         |11930|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|7           |11533|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|166         |10845|\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|236         |7913 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|223         |7542 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|238         |7318 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|82          |7292 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|181         |7282 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|95          |7244 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|244         |6733 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|61          |6606 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|116         |6339 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|138         |6144 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|97          |6050 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|49          |5221 |\n",
      "|{2024-03-14 11:55:00, 2024-03-14 12:00:00}|151         |5153 |\n",
      "+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/ppathorn/.pyenv/versions/3.11.4/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m popular_destinations \\\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39moutputMode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsole\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtruncate\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/homework6/.venv/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:221\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/homework6/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/Downloads/homework6/.venv/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/Downloads/homework6/.venv/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.4/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Error receiving network data closing socket\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/kafka/conn.py\", line 1090, in _recv\n",
      "    data = self._sock.recv(self.config['sock_chunk_bytes'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Error receiving network data closing socket\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/kafka/conn.py\", line 1090, in _recv\n",
      "    data = self._sock.recv(self.config['sock_chunk_bytes'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:<BrokerConnection node_id=1 host=localhost:9092 <connected> [IPv4 ('127.0.0.1', 9092)]>: Error receiving network data closing socket\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/ppathorn/Downloads/homework6/.venv/lib/python3.11/site-packages/kafka/conn.py\", line 1090, in _recv\n",
      "    data = self._sock.recv(self.config['sock_chunk_bytes'])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:20 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:21 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:22 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:23 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:23 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:24 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:26 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:27 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:28 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:29 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:30 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:31 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:32 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:33 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:34 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:35 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n",
      "24/03/14 12:00:36 WARN NetworkClient: [AdminClient clientId=adminclient-2] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.\n",
      "ERROR:kafka.conn:Connect attempt to <BrokerConnection node_id=1 host=localhost:9092 <connecting> [IPv4 ('127.0.0.1', 9092)]> returned error 61. Disconnecting.\n",
      "WARNING:kafka.client:Node 1 connection failed -- refreshing metadata\n"
     ]
    }
   ],
   "source": [
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
